{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTRn88RCkHCx",
        "outputId": "8a89dbe2-cdb9-46ab-aeb7-d30c7dd40054"
      },
      "outputs": [],
      "source": [
        "!pip install miditok\n",
        "# Installa le librerie necessarie\n",
        "!pip install pandas numpy tqdm\n",
        "!pip install music21 tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVQ1Mz34Eggx"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import music21\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import math\n",
        "from pathlib import Path\n",
        "\n",
        "# Import Keras/TensorFlow\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dropout, Dense, Activation, BatchNormalization, Embedding\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras.utils import Sequence, to_categorical\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import mixed_precision\n",
        "import tensorflow as tf\n",
        "# Import per il processing\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import per la generazione\n",
        "from music21 import converter, instrument, note, chord, stream\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFHlwqX0Ecx-",
        "outputId": "e689811d-bd4c-4968-fcbd-91b04468f23b"
      },
      "outputs": [],
      "source": [
        "\n",
        "data_dir = Path('data/maestro-v3.0.0')\n",
        "if not data_dir.exists():\n",
        "  tf.keras.utils.get_file(\n",
        "      'maestro-v3.0.0-midi.zip',\n",
        "      origin='https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0-midi.zip',\n",
        "      extract=True,\n",
        "      cache_dir='.', cache_subdir='data',\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rdPwrabKw8G",
        "outputId": "baa8e166-85c1-4337-f698-e711cf3184c2"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/data/maestro-v3.0.0/maestro-v3.0.0.csv', sep=\",\")\n",
        "autori = df.groupby(\"canonical_composer\").size().reset_index(name=\"count\").sort_values(by=\"count\", ascending=False)\n",
        "# Filtra il DataFrame 'autori' per mantenere solo le righe dove 'count' √® > 10\n",
        "autori_con_piu_di_10_brani = autori[autori['count'] > 10]\n",
        "\n",
        "# Visualizza il risultato\n",
        "print(autori_con_piu_di_10_brani)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2y2nIpKhK4u"
      },
      "outputs": [],
      "source": [
        "STILE_MAP = {\n",
        "    'Johann Sebastian Bach': 'STYLE_BAROQUE', 'Domenico Scarlatti': 'STYLE_BAROQUE',\n",
        "    'Ludwig van Beethoven': 'STYLE_CLASSICAL', 'Wolfgang Amadeus Mozart': 'STYLE_CLASSICAL',\n",
        "    'Joseph Haydn': 'STYLE_CLASSICAL',\n",
        "    'Fr√©d√©ric Chopin': 'STYLE_ROMANTIC', 'Franz Schubert': 'STYLE_ROMANTIC'\n",
        "}\n",
        "\n",
        "# --- LA CORREZIONE CHIAVE √à QUI ---\n",
        "NUM_MIDI_FOR_COMPOSER_MAP = {\n",
        "    'Johann Sebastian Bach': 120, 'Domenico Scarlatti': 31,\n",
        "    'Ludwig van Beethoven': 80, 'Wolfgang Amadeus Mozart': 38,\n",
        "    'Joseph Haydn': 40,\n",
        "    'Fr√©d√©ric Chopin': 80, 'Franz Schubert': 80\n",
        "}\n",
        "\n",
        "# Assicurati che i nomi qui corrispondano esattamente a quelli in 'canonical_composer' nel tuo CSV!\n",
        "COMPOSITORI_SELEZIONATI = list(STILE_MAP.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plKkDZkahk7-"
      },
      "outputs": [],
      "source": [
        "LOCAL_PATH = Path('/content/data/maestro-v3.0.0')\n",
        "# AGGIORNA I PERCORSI PER PUNTARLI AL TUO DRIVE\n",
        "BASE_DIR = Path('/content/drive/MyDrive/maestro-project/LLM') # <-- ESEMPIO, MODIFICA QUI\n",
        "METADATA_FILE = LOCAL_PATH / 'maestro-v3.0.0.csv'\n",
        "OUTPUT_DIR = BASE_DIR # <-- ESEMPIO, MODIFICA QUI\n",
        "\n",
        "# Aggiorna anche i percorsi di output globali\n",
        "TOKENIZED_MIDI_SONGS_PATH = OUTPUT_DIR / \"all_tokenized_songs.json\"\n",
        "TOKENIZER_SAVE_PATH = OUTPUT_DIR / \"tokenizer.json\"\n",
        "TRAIN_BIN_PATH = OUTPUT_DIR / \"train.bin\"\n",
        "VAL_BIN_PATH = OUTPUT_DIR / \"val.bin\"\n",
        "META_PKL_PATH = OUTPUT_DIR / \"meta.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5203fde7",
        "outputId": "2bb2737a-00cc-406f-aa96-dcf3bd1a61b2"
      },
      "outputs": [],
      "source": [
        "!cat /content/model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA6r0_OX3Ejx"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "from miditok import TokenizerConfig, REMI\n",
        "\n",
        "METADATA_FILE = LOCAL_PATH / 'maestro-v3.0.0.csv'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC7hLZzS9Ues",
        "outputId": "a202bfbc-179d-4e57-8982-09ddfe440f5f"
      },
      "outputs": [],
      "source": [
        "SPECIAL_TOKENS = [\"PAD_TOKEN\", \"BOS_TOKEN\", \"EOS_TOKEN\", \"MASK_TOKEN\"] + list(set(STILE_MAP.values()))\n",
        "print(SPECIAL_TOKENS)\n",
        "TOKENIZER_CONFIGV2 = TokenizerConfig(special_tokens=SPECIAL_TOKENS)\n",
        "tokenizer = REMI(tokenizer_config=TOKENIZER_CONFIGV2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5J7JP64-wRa"
      },
      "outputs": [],
      "source": [
        "def midi_filter_tokenizzation():\n",
        "  metadata = pd.read_csv(METADATA_FILE)\n",
        "  metadata_filtrati = metadata[metadata['canonical_composer'].isin(STILE_MAP.keys())].copy()\n",
        "  metadata_filtrati['style'] = metadata_filtrati['canonical_composer'].map(STILE_MAP)\n",
        "  print(metadata_filtrati.head())\n",
        "\n",
        "  metadata_campionati = metadata_filtrati.groupby('canonical_composer').apply(\n",
        "      lambda group: group.sample(\n",
        "          n = min(NUM_MIDI_FOR_COMPOSER_MAP[group.name], len(group)),\n",
        "          random_state = 42\n",
        "      ))\n",
        "   # --- VERIFICA (Come prima) ---\n",
        "  print(\"Numero di brani per compositore dopo il campionamento casuale:\")\n",
        "  print(metadata_campionati['canonical_composer'].value_counts())\n",
        "  print(\"-\" * 30)\n",
        "\n",
        "  file_midi_da_processare = metadata_campionati['midi_filename'].tolist()\n",
        "  map_midi2style = metadata_campionati.set_index(\"midi_filename\")[\"style\"].to_dict()\n",
        "  all_tokenized_song = []\n",
        "  print(f\"üéº Trovati {len(file_midi_da_processare)} file per i compositori: {', '.join(COMPOSITORI_SELEZIONATI)}\")\n",
        "  if not file_midi_da_processare:\n",
        "    return None\n",
        "  print(file_midi_da_processare)\n",
        "  for midi_filename in tqdm(file_midi_da_processare):\n",
        "    file_path = LOCAL_PATH / midi_filename\n",
        "    tokens = tokenizer(file_path)\n",
        "\n",
        "    full_seq =[tokenizer.vocab[\"BOS_TOKEN\"]] + [tokenizer.vocab[map_midi2style[midi_filename]]] + tokens[0].ids + [tokenizer.vocab[\"EOS_TOKEN\"]]\n",
        "    all_tokenized_song.append(full_seq)\n",
        "  return all_tokenized_song\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvfaL19J9ixQ"
      },
      "outputs": [],
      "source": [
        "def midi_tokenizzation(tokenizer):\n",
        "  metadata = pd.read_csv(METADATA_FILE)\n",
        "\n",
        "\n",
        "  file_midi_da_processare = metadata['midi_filename'].tolist()\n",
        "\n",
        "\n",
        "  all_tokenized_song = []\n",
        "  print(f\"üéº Trovati {len(file_midi_da_processare)} file MIDI totali nel dataset.\")\n",
        "  if not file_midi_da_processare:\n",
        "    return None\n",
        "\n",
        "  # Aggiungi la barra di progresso per i file MIDI\n",
        "  for midi_filename in tqdm(file_midi_da_processare, desc=\"Tokenizing MIDI files\"):\n",
        "    file_path = LOCAL_PATH / midi_filename\n",
        "    try:\n",
        "        tokens = tokenizer(file_path)\n",
        "\n",
        "        if tokens and tokens[0].ids: # Controlla se tokens e tokens[0].ids non sono vuoti\n",
        "            full_seq = [tokenizer.vocab[\"BOS_TOKEN\"]] + tokens[0].ids + [tokenizer.vocab[\"EOS_TOKEN\"]]\n",
        "            all_tokenized_song.append(full_seq)\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Skipping {midi_filename}: Could not tokenize or empty sequence.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Errore durante la tokenizzazione di {midi_filename}: {e}\")\n",
        "        continue # Continua con il prossimo file in caso di errore\n",
        "\n",
        "  print(f\"‚úÖ Tokenizzazione completata. Elaborati {len(all_tokenized_song)} brani.\")\n",
        "  return all_tokenized_song"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MupChPw_Mzd",
        "outputId": "eebfa4bc-0cfc-4b1b-d9fa-eaf27240b9e3"
      },
      "outputs": [],
      "source": [
        "all_tokenized_song = midi_filter_tokenizzation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2dZB69N3gQR"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def save_tokenized_midi(file_path, all_tokenized_song):\n",
        "\n",
        "  print(f\"üíæ Salvataggio di {len(all_tokenized_song)} brani tokenizzati in {file_path}...\")\n",
        "  with open(file_path, 'w') as f:\n",
        "    json.dump(all_tokenized_song, f)\n",
        "  print(\"Salvataggio completato!\")\n",
        "\n",
        "def load_tokenized_midi(file_path):\n",
        "\n",
        "  print(f\"üìÇ Caricamento dei brani tokenizzati da {file_path}...\")\n",
        "\n",
        "  with open(file_path, 'r') as f:\n",
        "    loaded_tokenized_songs = json.load(f)\n",
        "\n",
        "  print(f\"Caricati {len(loaded_tokenized_songs)} brani.\")\n",
        "  return loaded_tokenized_songs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OBpyhYAvuL_"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "songs = all_tokenized_song\n",
        "# Shuffle i brani per non avere bias di ordine\n",
        "random.shuffle(songs)\n",
        "\n",
        "n = len(songs)\n",
        "train_split = int(0.8 * n)\n",
        "val_split = int(0.9 * n)\n",
        "\n",
        "train_songs = songs[:train_split]\n",
        "val_songs = songs[train_split:val_split]\n",
        "test_songs = songs[val_split:]\n",
        "\n",
        "def flatten(song_list):\n",
        "    tokens = []\n",
        "    for song in song_list:\n",
        "        tokens.extend(song)\n",
        "    return tokens\n",
        "\n",
        "all_tokens_train = flatten(train_songs)\n",
        "all_tokens_val = flatten(val_songs)\n",
        "all_tokens_test = flatten(test_songs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZX1jwSjoIH6u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MidiDataset(Dataset):\n",
        "    def __init__(self, tokens, seq_len=512):\n",
        "        self.tokens = tokens\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.tokens[idx : idx + self.seq_len]\n",
        "        y = self.tokens[idx + 1 : idx + self.seq_len + 1]\n",
        "        return torch.tensor(x), torch.tensor(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3hdEY7aFcay"
      },
      "outputs": [],
      "source": [
        "train_dataset = MidiDataset(all_tokens_train, seq_len=1024)\n",
        "val_dataset   = MidiDataset(all_tokens_val, seq_len=1024)\n",
        "test_dataset  = MidiDataset(all_tokens_test, seq_len=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiJ49Bi6ww_o",
        "outputId": "f920b1a9-6849-4404-e387-3c399f6e8bef"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE =  256\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0xB21mMra_r",
        "outputId": "672dc4cf-b3f3-49e3-91bb-128cc8a5248a"
      },
      "outputs": [],
      "source": [
        "tokenizer.vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiLdMfJPYIi2",
        "outputId": "4ba2b7e0-b4b2-4a42-d8cd-325b304daa30"
      },
      "outputs": [],
      "source": [
        "from model import GPT, GPTConfig\n",
        "config = GPTConfig(\n",
        "    block_size=1024,      # Lunghezza massima della sequenza di input\n",
        "    vocab_size=tokenizer.vocab_size,    # Dimensione del vocabolario\n",
        "    n_layer=6,           # Numero di blocchi Transformer\n",
        "    n_head=6,            # Numero di \"teste\" di attenzione\n",
        "    n_embd=96,           # Dimensione dei vettori di embedding\n",
        "    dropout=0.1,\n",
        "    bias = False\n",
        ")\n",
        "\n",
        "\n",
        "model = GPT(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOvn3P0FbDUP"
      },
      "outputs": [],
      "source": [
        "model = torch.compile(model)\n",
        "\n",
        "# Ottimizzatore AdamW\n",
        "learning_rate = 1e-3\n",
        "max_iters = 5000\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD3g-V0lpj0I",
        "outputId": "fe0a1b53-c4e7-49d9-ed41-faa064050ec8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "# --- CONFIGURAZIONE ---\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.set_float32_matmul_precision('high')  # Tensor Cores\n",
        "\n",
        "# Hyperparametri\n",
        "learning_rate = 5e-4\n",
        "weight_decay = 0.1\n",
        "beta1, beta2 = 0.9, 0.95\n",
        "max_epochs = 10\n",
        "batch_size = 512              # batch sicuro\n",
        "grad_accum_steps = 2          # simula batch 512\n",
        "grad_clip_norm = 1.0\n",
        "eval_interval = 1\n",
        "\n",
        "# --- DATALOADERS ---\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                          num_workers=8, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                        num_workers=4, pin_memory=True)\n",
        "\n",
        "# --- MODELLO ---\n",
        "model.to(device)\n",
        "print(f\"Modello spostato su: {device}\")\n",
        "\n",
        "# --- OTTIMIZZATORE ---\n",
        "optimizer = model.configure_optimizers(weight_decay=weight_decay,\n",
        "                                       learning_rate=learning_rate,\n",
        "                                       betas=(beta1, beta2),\n",
        "                                       device_type=device)\n",
        "\n",
        "# --- SCHEDULER LR ---\n",
        "total_steps = max_epochs * len(train_loader) // grad_accum_steps\n",
        "warmup_steps = max(50, int(0.01 * total_steps))\n",
        "\n",
        "def lr_lambda(current_step):\n",
        "    if current_step < warmup_steps:\n",
        "        return float(current_step) / float(max(1, warmup_steps))\n",
        "    progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
        "    return 0.5 * (1.0 + np.cos(np.pi * progress))\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# --- Mixed precision scaler ---\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# --- FUNZIONE DI STIMA LOSS ---\n",
        "@torch.no_grad()\n",
        "def estimate_metrics(): # Rinominata per chiarezza\n",
        "    metrics = {}\n",
        "    model.eval()\n",
        "    for split_name, loader in [('train', train_loader), ('val', val_loader)]:\n",
        "        losses = []\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for i, (X, Y) in enumerate(loader):\n",
        "            if i >= 50:  # limita valutazione per velocit√†\n",
        "                break\n",
        "            X, Y = X.to(device, non_blocking=True), Y.to(device, non_blocking=True)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                logits, loss = model(X, Y)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # Calcolo dell'accuracy\n",
        "            preds = torch.argmax(logits, dim=2) # Changed dim from 1 to 2\n",
        "            correct_predictions += (preds.view(-1) == Y.view(-1)).sum().item() # Flatten tensors before comparison\n",
        "            total_samples += Y.numel() # Use numel() for total number of elements\n",
        "\n",
        "        metrics[f'{split_name}_loss'] = np.mean(losses)\n",
        "        metrics[f'{split_name}_accuracy'] = correct_predictions / total_samples\n",
        "\n",
        "    model.train()\n",
        "    return metrics\n",
        "\n",
        "# --- CHECKPOINT ---\n",
        "best_val_loss = float('inf')\n",
        "checkpoint_path = OUTPUT_DIR / 'CheckPoint'\n",
        "checkpoint_path.mkdir(parents=True, exist_ok=True) # Create the directory if it doesn't exist\n",
        "checkpoint_file = checkpoint_path / 'best_model.pt' # Define the checkpoint file name\n",
        "\n",
        "# --- TRAINING LOOP ---\n",
        "print(\"üöÄ Inizio dell'addestramento...\")\n",
        "start_time = time.time()\n",
        "step_timer = start_time\n",
        "global_step = 0\n",
        "optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    model.train()\n",
        "    for i, (X, Y) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{max_epochs}\")):\n",
        "        X, Y = X.to(device, non_blocking=True), Y.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / grad_accum_steps  # scala per accumulation\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Gradient accumulation\n",
        "        if (i + 1) % grad_accum_steps == 0 or (i + 1) == len(train_loader):\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            scheduler.step()\n",
        "            global_step += 1\n",
        "\n",
        "            # Logging step/sec e VRAM\n",
        "            if global_step % 10 == 0:\n",
        "                elapsed = time.time() - step_timer\n",
        "                step_timer = time.time()\n",
        "                mem_alloc = torch.cuda.memory_allocated() / 1024**2\n",
        "                mem_total = torch.cuda.get_device_properties(device).total_memory / 1024**2\n",
        "                current_lr = scheduler.get_last_lr()[0]\n",
        "                print(f\"Step {global_step}, {10/elapsed:.2f} step/sec, VRAM {mem_alloc:.1f}/{mem_total:.1f} MB, LR {current_lr:.2e}\")\n",
        "\n",
        "    # Valutazione periodica\n",
        "    if epoch % eval_interval == 0 or epoch == max_epochs - 1:\n",
        "        metrics = estimate_metrics()\n",
        "        current_val_loss = metrics['val_loss']\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(\"-\" * 70)\n",
        "        print(f\"Epoch {epoch+1}: train loss {metrics['train_loss']:.4f}, val loss {current_val_loss:.4f}, \"\n",
        "              f\"train acc {metrics['train_accuracy']:.2%}, val acc {metrics['val_accuracy']:.2%}, \"\n",
        "              f\"tempo: {elapsed_time:.2f}s\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        # Checkpoint se migliora\n",
        "        if current_val_loss < best_val_loss:\n",
        "            best_val_loss = current_val_loss\n",
        "            print(f\"üéâ Nuova val_loss migliore: {best_val_loss:.4f}. Salvataggio checkpoint...\")\n",
        "            checkpoint = {\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'config': config,\n",
        "                'tokenizer_vocab': tokenizer.vocab\n",
        "            }\n",
        "            torch.save(checkpoint, checkpoint_file) # Use checkpoint_file for saving\n",
        "        else:\n",
        "            print(f\"Val_loss ({current_val_loss:.4f}) non migliorata rispetto a {best_val_loss:.4f}.\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "print(\"‚úÖ Addestramento completato.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsKMvFVJJrlJ",
        "outputId": "0ef48f03-5bab-4bde-d80f-6f21b48edae6"
      },
      "outputs": [],
      "source": [
        "# Salva il file MIDI\n",
        "output_midi_path = OUTPUT_DIR / \"generated_composition.mid\"\n",
        "generated_midi.dump_midi(output_midi_path)\n",
        "\n",
        "print(f\"\\nComposizione generata e salvata in: {output_midi_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7030f05",
        "outputId": "b7bcb41e-8098-416f-e619-6549c94d0922"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# Definisci il percorso dove salvare il modello\n",
        "# Puoi modificarlo se vuoi salvare in un'altra posizione o con un nome diverso\n",
        "model_save_path = OUTPUT_DIR / \"final_model_state_dict.pt\"\n",
        "\n",
        "# Assicurati che la directory di destinazione esista\n",
        "model_save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Salva lo stato del modello\n",
        "# Stiamo salvando solo lo state_dict, che √® sufficiente per ricaricare il modello\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"Stato del modello salvato in: {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWsl_myfyRcw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_midi(model, start_token, max_len=1024, temperature=1.0, top_k=None, device='cpu'):\n",
        "    \"\"\"\n",
        "    Genera una sequenza di token MIDI a partire da start_token.\n",
        "\n",
        "    Args:\n",
        "        model: il modello Transformer gi√† allenato\n",
        "        start_token: intero del token iniziale (<BOS>)\n",
        "        max_len: lunghezza massima della sequenza generata\n",
        "        temperature: controlla casualit√† (1.0 = normale, <1 pi√π conservativo, >1 pi√π creativo)\n",
        "        top_k: se non None, prende solo i k token pi√π probabili a ogni step\n",
        "        device: 'cpu' o 'cuda'\n",
        "\n",
        "    Returns:\n",
        "        lista di token generati (incluso start_token e )\"\"\"\n",
        "    model.eval()\n",
        "    sequence = [start_token]\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len - 1):\n",
        "            input_tensor = torch.tensor(sequence).unsqueeze(0).to(device)\n",
        "            output = model(input_tensor)\n",
        "            # Estrai i logit dall'output del modello. Se l'output √® una tupla, prendi il primo elemento.\n",
        "            logits = output[0] if isinstance(output, tuple) else output\n",
        "\n",
        "            # Prendi i logit solo per l'ultimo token generato\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1).squeeze(1).tolist()[0]\n",
        "            sequence.append(next_token)\n",
        "\n",
        "            # Interrompi se viene generato il token EOS\n",
        "            # if next_token == <EOS_TOKEN_VALUE>: # Sostituisci con il valore effettivo del token EOS\n",
        "            #     break\n",
        "\n",
        "    return sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ1PDK_mRPXM"
      },
      "source": [
        "#Gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!sudo apt-get install -y fluidsynth\n",
        "!wget https://github.com/FluidSynth/fluidsynth/raw/master/sf2/FluidR3_GM.sf2\n",
        "!pip install midi2audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhdSUMzLROfE"
      },
      "outputs": [],
      "source": [
        "checkpoint_gen = torch.load(\"/content/drive/MyDrive/maestro-project/LLM/CheckPoint/best_model.pt\", weights_only=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TegP1ebRZvb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from collections import OrderedDict\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "state_dict = checkpoint_gen\n",
        "\n",
        "# Crea un nuovo dizionario senza il prefisso\n",
        "clean_state_dict = OrderedDict()\n",
        "for k, v in state_dict.items():\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        # Rimuovi il prefisso\n",
        "        new_k = k[len(unwanted_prefix):]\n",
        "        clean_state_dict[new_k] = v\n",
        "    else:\n",
        "        # Se qualche chiave non ha il prefisso, tienila com'√®\n",
        "        clean_state_dict[k] = v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXTjbPSPlvOk",
        "outputId": "620f47de-e711-4546-cb9d-7050245e4ea5"
      },
      "outputs": [],
      "source": [
        "state_dict_loaded = checkpoint_gen['model_state_dict']\n",
        "\n",
        "# 5. Crea un nuovo dizionario e rimuovi il prefisso '_orig_mod.' da ogni chiave\n",
        "new_state_dict = OrderedDict()\n",
        "for k, v in state_dict_loaded.items():\n",
        "    # Usa .removeprefix() per un codice pi√π pulito e moderno (Python 3.9+)\n",
        "    # Se usi una versione pi√π vecchia di Python, usa: name = k[len('_orig_mod.'):]\n",
        "    name = k.removeprefix('_orig_mod.')\n",
        "    new_state_dict[name] = v\n",
        "\n",
        "# 6. Carica il state_dict corretto e pulito nel tuo modello\n",
        "model.load_state_dict(new_state_dict)\n",
        "\n",
        "print(\"Modello caricato con successo!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Doi0smmlSDC-",
        "outputId": "111f3a7a-94d3-427e-b280-70e9949b1947"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "# Sposta il modello sulla GPU se disponibile\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Modello pronto per l'inferenza su dispositivo: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzuOpaDNUcQq",
        "outputId": "00394f27-f7f1-4ef0-a1a3-73046a135a4b"
      },
      "outputs": [],
      "source": [
        "prompt_tokens = val_dataset[0][:100] # Prende le etichette (Y) dal primo elemento\n",
        "prompt_tokens = prompt_tokens[0][:1].tolist() + [6] + prompt_tokens[0][1:].tolist()\n",
        "print(prompt_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6GX3xIxYFAp",
        "outputId": "9249df79-d2e0-4214-9e57-bd5d24618426"
      },
      "outputs": [],
      "source": [
        "\n",
        "song = []\n",
        "for midi_filename in tqdm('/content/data/maestro-v3.0.0/2004/MIDI-Unprocessed_SMF_07_R1_2004_01_ORIG_MID--AUDIO_07_R1_2004_02_Track02_wav.midi'):\n",
        "    file_path = '/content/data/maestro-v3.0.0/2009/MIDI-Unprocessed_20_R1_2009_01-05_ORIG_MID--AUDIO_20_R1_2009_20_R1_2009_02_WAV.midi'\n",
        "    tokens = tokenizer(file_path)\n",
        "\n",
        "    full_seq =[tokenizer.vocab[\"BOS_TOKEN\"]] + [5] + tokens[0].ids + [tokenizer.vocab[\"EOS_TOKEN\"]]\n",
        "    song.append(full_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbIPNi4TZLQY",
        "outputId": "363b9634-05bd-4f8a-e750-5377a8752f99"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#song = np.random.randint(low=9, high=288, size=1024)\n",
        "# 2. Convertila in un tensore\n",
        "mio_tensore = torch.tensor(song[:1024])\n",
        "\n",
        "print(f\"La mia lista: {song}\")\n",
        "print(f\"Il mio tensore: {len(mio_tensore)}\")\n",
        "print(f\"Tipo di dato del tensore: {mio_tensore.dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB2x7jRjSOOS",
        "outputId": "be0fccc9-6cf3-4b75-c162-5f2c98408cd5"
      },
      "outputs": [],
      "source": [
        "prompt_tokens = val_dataset[0][:300] # Prende le etichette (Y) dal primo elemento\n",
        "prompt_tokens = mio_tensore\n",
        "# Converti in un tensore di PyTorch con la dimensione del batch (1)\n",
        "# Seleziona il primo elemento della tupla prompt_tokens\n",
        "prompt_tensor = torch.tensor(prompt_tokens[0], dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "\n",
        "print(\"\\nPrompt iniziale (decodificato):\")\n",
        "# Wrap the list of token IDs in another list\n",
        "print(tokenizer.decode([prompt_tensor.squeeze(0).tolist()]))\n",
        "print(\"-\" * 50)\n",
        "print(\"Inizio generazione...\\n\")\n",
        "\n",
        "# --- GENERAZIONE AUTOREGRESSIVA ---\n",
        "@torch.no_grad() # Disabilita il calcolo dei gradienti per l'inferenza\n",
        "def generate(model, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "    \"\"\"\n",
        "    Funzione di generazione di testo.\n",
        "    idx: tensore (B, T) di indici di token nel contesto attuale\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Se il contesto diventa troppo lungo, taglialo per adattarlo alla block_size del modello\n",
        "        block_size = model.config.block_size # Assumendo che la config abbia block_size\n",
        "        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
        "\n",
        "        # Forward pass per ottenere i logits per l'ultimo token\n",
        "        logits, _ = model(idx_cond) # Ignoriamo la loss durante l'inferenza\n",
        "        logits = logits[:, -1, :] # Prendi solo i logits per l'ultimo step temporale -> (B, vocab_size)\n",
        "\n",
        "        # Applica la temperatura\n",
        "        logits = logits / temperature\n",
        "\n",
        "        # (Opzionale) Applica Top-k sampling\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf') # Metti a -infinito i logits non nel top-k\n",
        "\n",
        "        # Converti i logits in probabilit√†\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "        # Campiona il token successivo dalla distribuzione di probabilit√†\n",
        "        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "        # Aggiungi il nuovo token alla sequenza\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "# Esegui la generazione\n",
        "# Definisci max_new_tokens, temperature e top_k (puoi usare i valori che preferisci)\n",
        "max_new_tokens = 1024  # Esempio: genera 200 nuovi token\n",
        "temperature = 0.9     # Esempio: temperatura per il campionamento\n",
        "top_k = 20            # Esempio: usa top-k sampling con k=50\n",
        "\n",
        "generated_tokens_tensor = generate(model, prompt_tensor, max_new_tokens, temperature, top_k)\n",
        "\n",
        "# Estrai solo i tokens generati (escludendo il prompt)\n",
        "generated_ids = generated_tokens_tensor.squeeze(0).tolist()[len(prompt_tokens):] # Adjusted index here\n",
        "\n",
        "print(\"Token ID generati:\", generated_ids) # Print generated token IDs\n",
        "\n",
        "# Filter out tokens outside the vocabulary size\n",
        "vocab_size = len(tokenizer.vocab)\n",
        "filtered_generated_ids = [t for t in generated_ids if t < vocab_size]\n",
        "\n",
        "# Wrap the list of filtered token IDs in another list and decode\n",
        "generated_text = tokenizer.decode([filtered_generated_ids])\n",
        "\n",
        "# --- STAMPA IL RISULTATO ---\n",
        "print(\"Testo generato:\")\n",
        "print(generated_text)\n",
        "print(\"-\" * 50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY5hzJkKUKVr"
      },
      "outputs": [],
      "source": [
        "# Assign filtered_generated_ids to generated_ids for the next cell (Cg0D20LH0hyH)\n",
        "generated_ids = filtered_generated_ids\n",
        "\n",
        "midi_obj = tokenizer.decode([generated_ids]) # Use the filtered generated_ids\n",
        "midi_obj.dump_midi('/content/sample_data/mus.mid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "id": "asqjsDWzTVKE",
        "outputId": "6dab89af-de9a-49c9-d569-47bf237c9239"
      },
      "outputs": [],
      "source": [
        "from midi2audio import FluidSynth\n",
        "from IPython.display import Audio\n",
        "\n",
        "# Chuy·ªÉn MIDI sang WAV\n",
        "fs = FluidSynth()\n",
        "fs.midi_to_audio('/content/sample_data/mus.mid', 'output.wav')\n",
        "\n",
        "# Ph√°t WAV file\n",
        "Audio('output.wav')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "e6c5d3ed",
        "outputId": "1b9d36c0-e3f5-4276-a710-1ed2cfc152ed"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Scarica il file WAV generato\n",
        "files.download('output.wav')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvu9Bl5ZRRfP"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
